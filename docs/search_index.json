[["index.html", "1 Welcome to S2Y Lab 8 1.1 Introduction", " S2Y Lab 8 Comparing regression lines 1 Welcome to S2Y Lab 8 Intended Learning Outcomes: use R to carry out an ANCOVA; use R output for model selection when comparing regression lines; and interpret confidence intervals for model selection when comparing regression lines. 1.1 Introduction In Chapter 4 of the lectures we looked at how to select the best linear model in the case where we have two explanatory variables, one of which is a grouping variable (factor). Three models were considered: (i) different regression lines for each group; (ii) parallel regression lines; and (iii) a single regression line. Selection was based on the calculation and interpretation of confidence intervals. Here is a reminder of some of the formulae: Model 1: Two different regression lines \\[\\mathbb{E}(Y_{ij}) = \\alpha_i+\\beta_i(x_{ij}-\\bar{x}_{i.}), \\quad i=1,2, \\quad j=1,\\ldots,n_i\\] The least squares estimates of the parameters are \\[\\hat{\\boldsymbol \\beta} = \\begin{bmatrix} \\hat{\\alpha}_1 \\\\[0.3em] \\hat{\\beta}_1 \\\\[0.3em] \\hat{\\alpha}_2 \\\\[0.3em] \\hat{\\beta}_2\\end{bmatrix} = \\begin{bmatrix} \\bar{y}_{1.} \\\\[0.3em] \\frac{S_{x_1 y_1}}{S_{x_1 x_1}} \\\\[0.3em] \\bar{y}_{2.} \\\\[0.3em] \\frac{S_{x_2 y_2}}{S_{x_2 x_2}}\\end{bmatrix}\\] and the residual sum of squares can be written as \\[\\text{RSS} = \\text{RSS}_1 + \\text{RSS}_2,\\] where \\[\\text{RSS}_1 = S_{y_1 y_1} - \\frac{\\left(S_{x_1 y_1}\\right)^2}{S_{x_1 x_1}} \\quad\\quad \\text{and} \\quad\\quad \\text{RSS}_2 = S_{y_2 y_2} - \\frac{\\left(S_{x_2 y_2}\\right)^2}{S_{x_2 x_2}}.\\] The equations for the parameters are identical to those which are found when a separate straight line is fitted individually to the two groups, with \\(\\text{RSS}_1\\) and \\(\\text{RSS}_2\\) the residual sums of squares from fitting individual regression lines to each group. Recall that in order to determine if a model with parallel lines is adequate, we need to construct a 95% confidence interval for \\((\\beta_1 - \\beta_2)\\) using the standard formula: \\[\\mathbf{b}^\\top\\boldsymbol{\\hat{\\beta}} \\pm t(n-p; 0.975)\\sqrt{\\frac{\\text{RSS}}{n-p}\\mathbf{b}^\\top(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{b}},\\] That is, \\[(\\hat{\\beta}_1-\\hat{\\beta}_2) \\pm t(n_1+n_2-4; 0.975)\\sqrt{\\left(\\frac{\\text{RSS}_1+\\text{RSS}_2}{n_1+n_2-4}\\right)\\left(\\frac{1}{S_{x_1x_1}}+\\frac{1}{S_{x_2x_2}}\\right)}.\\] If the confidence interval for \\(\\beta_1 - \\beta_2\\) includes 0, there is insufficient evidence of a difference in the slopes. We will go on to fit the parallel lines model. Model 2: Two parallel regression lines \\[\\mathbb{E}(Y_{ij}) = \\alpha_i+\\beta(x_{ij}-\\bar{x}_{i.}), \\quad i=1,2, \\quad j=1,\\ldots,n_i\\] Here the least squares estimates of the parameters are \\[\\hat{\\boldsymbol \\beta} = \\begin{bmatrix} \\hat{\\alpha}_1 \\\\[0.3em] \\hat{\\alpha}_2 \\\\[0.3em] \\hat{\\beta}\\end{bmatrix} = \\begin{bmatrix} \\bar{y}_{1.} \\\\[0.3em] \\bar{y}_{2.} \\\\[0.3em] \\frac{S_{x_1 y_1} + S_{x_2 y_2}}{S_{x_1 x_1} + S_{x_2 x_2}}\\end{bmatrix}\\] and the residual sum of squares is given as \\[\\text{RSS} = S_{y_1 y_1} + S_{y_2 y_2} - \\frac{\\left(S_{x_1 y_1} + S_{x_2 y_2}\\right)^2}{S_{x_1 x_1} + S_{x_2 x_2}}.\\] By constructing a confidence interval for the difference between the regression lines, \\(\\alpha_1 - \\alpha_2 + \\beta \\left(\\bar{x}_{2.} - \\bar{x}_{1.}\\right)\\), and examining whether this interval includes 0, we can assess whether a single straight line, with no difference between the groups, is adequate for the data. The 95% confidence interval has the form \\[\\mathbf{b}^\\top\\boldsymbol{\\hat{\\beta}} \\pm t(n_1 + n_2 - 3; 0.975)\\sqrt{\\frac{\\text{RSS}}{n_1 + n_2 - 3}\\mathbf{b}^\\top(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{b}},\\] with \\(\\mathbf{b}^\\top = \\begin{bmatrix} 1 &amp; -1 &amp; \\left(\\bar{x}_{2.} - \\bar{x}_{1.}\\right) \\end{bmatrix}\\). That is, \\[\\hat{\\alpha}_1-\\hat{\\alpha}_2+\\hat{\\beta}(\\bar{x}_{2.}-\\bar{x}_{1.}) \\pm t(n_1 + n_2 - 3,0.975) \\sqrt{\\left(\\frac{\\text{RSS}}{n_1 + n_2 - 3}\\right)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}+\\frac{(\\bar{x}_{2.}-\\bar{x}_{1.})^2}{S_{x_1x_1}+S_{x_2x_2}}\\right)}.\\] If the confidence interval includes 0, there is insufficient evidence of a difference between the regression lines. We will then go on to fit a single regression line. Model 3: Single regression line \\[\\mathbb{E}(Y_{ij}) = \\alpha + \\beta(x_{ij}-\\bar{x}_{i.}), \\quad i=1,2, \\quad j=1,\\ldots,n_i\\] The parameter estimates are then \\[\\hat{\\boldsymbol \\beta} = \\begin{bmatrix} \\hat{\\alpha} \\\\[0.3em] \\hat{\\beta}\\end{bmatrix} = \\begin{bmatrix} \\bar{y}_{..} \\\\[0.3em] \\frac{S_{x y}}{S_{x x}}\\end{bmatrix}\\] In this lab the main focus will be on how to perform an analysis of covariance (ANCOVA) in R. However, we will also consider the interpretation of the above confidence intervals. Performing ANCOVA in R will provide \\(p\\)-values, which allow us to compare the three models of interest. "],["example-1-cholesterol-levels-in-iowa-and-nebraska.html", "2 Example 1: Cholesterol levels in Iowa and Nebraska 2.1 Exploratory analysis 2.2 Statistical analysis 2.3 Confidence intervals", " 2 Example 1: Cholesterol levels in Iowa and Nebraska In a survey to examine relationships between the nutrition and the health of women in the Mid-West of the USA, the concentration of cholesterol (mg/dL) in blood serum was measured on randomly selected subjects in Iowa and Nebraska. Cholesterol is known to depend on age for these subjects. Interest lies in whether there are any differences between the measurements in Iowa and those in Nebraska. Read in the data using: cholest &lt;- read.csv(&quot;cholest2.csv&quot;) The dataset contains four columns:    C1: Age    C2: Cholesterol level    C3: State (1 = Iowa, 2 = Nebraska)    C4: State name Question of interest: Are there any differences between the cholesterol measurements in Iowa and those in Nebraska? 2.1 Exploratory analysis Produce a plot of cholesterol against age, labelled by state, using the commands: plot(Cholesterol ~ Age, pch = State, data = cholest, col = State, xlab = &quot;Age (years)&quot;, ylab = &quot;Cholesterol (mg/dl)&quot;, cex = 1.5) legend(&quot;topleft&quot;, legend = c(&quot;Iowa&quot;, &quot;Nebraska&quot;), pch = 1:2, horiz = TRUE, bty = &quot;n&quot;, cex = 1.2, col = 1:2) Figure 2.1: Scatterplot of cholesterol level against age. COMMENT: What can you conclude from Figure 2.1? Do you think that the relationship between cholesterol and age is completely different in each state? Do you think either a parallel or single line relationship may be appropriate? 2.2 Statistical analysis Initially, a model can be fitted in R that allows the relationship between age and cholesterol to be completely different for each state. This can be done by using the command: Model1 &lt;- lm(Cholesterol ~ Age * statename, data = cholest) This fits a linear regression model between cholesterol and age where the relationship is allowed to change depending on state. Therefore, the model contains a covariate of age, a factor of state name and the interaction between state name and age. If the model with two completely separate regression lines is appropriate then we say that there is an interaction (and the interaction term would be statistically significant). An interaction would tell us that the relationship between cholesterol and age is different depending on whether the population of interest is Iowa or Nebraska. In order to assess this, the output of most interest to us here is in the analysis of variance table obtained using: anova(Model1) ## Analysis of Variance Table ## ## Response: Cholesterol ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Age 1 48976 48976 26.3124 2.388e-05 *** ## statename 1 5456 5456 2.9315 0.09877 . ## Age:statename 1 709 709 0.3809 0.54247 ## Residuals 26 48395 1861 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 QUESTION: Based on the ANOVA table, what can we say about the significance of the interaction term? The \\(p\\)-value for Age is 2.388e-05, and thus the interaction term is statistically significant. The \\(p\\)-value for statename is 0.09877, and thus the interaction term is statistically insignificant. The \\(p\\)-value for Age:statename is 0.54247, and thus the interaction term is statistically insignificant. What does the significance of the interaction term tell us about the model selection? We should choose different regression lines. We should fit parallel regression lines and test if this is appropriate. We should fit a single regression line and test if this is appropriate. Since the interaction term is not statistically significant (0.542) we then fit a model which assumes parallel lines: Model2 &lt;- lm(Cholesterol ~ Age + statename, data = cholest) anova(Model2) ## Analysis of Variance Table ## ## Response: Cholesterol ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Age 1 48976 48976 26.9298 1.831e-05 *** ## statename 1 5456 5456 3.0003 0.09466 . ## Residuals 27 49104 1819 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 QUESTION: Which of the following interpretations is appropriate about the ANOVA table? The \\(p\\)-value for Age is statistically significant, and hence the current model is appropriate. The \\(p\\)-value for Age is statistically insignificant, and hence we should fit a single regression line and check if this model is appropriate. The \\(p\\)-value for statename is statistically significant, and hence the current model is appropriate. The \\(p\\)-value for statename is statistically insignificant, and hence we should fit a single regression line and check if this model is appropriate. Since the \\(p\\)-value for the factor statename is not statistically significant (0.095), a single regression line may be appropriate for the data. QUESTION: Fit a single regression line to this dataset and comment on its appropriateness. Hint This is simply the simple linear regression model we learned about in previous weeks. Fit using lm and Cholesterol as the response variable and Age as the predictor variable. Solution Model3 &lt;- lm(Cholesterol ~ Age, data = cholest) anova(Model3) ## Analysis of Variance Table ## ## Response: Cholesterol ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Age 1 48976 48976 25.134 2.673e-05 *** ## Residuals 28 54560 1949 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the \\(p\\)-value for Age is 2.673e-05, which is much smaller than 0.05, we can conclude that Age is useful in predicting cholesterol level and hence a single regression line is appropriate. The final model (Model3) regression line can be superimposed onto Figure 2.1 using abline(Model3) plot(Cholesterol ~ Age, pch = State, data = cholest, col = State, xlab = &quot;Age (years)&quot;, ylab = &quot;Cholesterol (mg/dl)&quot;, cex = 1.5) legend(&quot;topleft&quot;, legend = c(&quot;Iowa&quot;, &quot;Nebraska&quot;), pch = 1:2, horiz = TRUE, bty = &quot;n&quot;, cex = 1.2, col = 1:2) abline(Model3) Figure 2.2: Scatterplot of cholesterol level against age. Figure 2.2 shows the fitted regression line from Model3 superimposed onto the scatterplot of cholesterol level and age. While skipped here, the assumptions of this simple linear regression model should be checked. The above analysis could also be performed by computing confidence intervals to investigate differences between the regression lines. 2.3 Confidence intervals Confidence intervals can be used to compare the models. To compare the full model with completely separate lines to the parallel lines model, we compute a confidence interval that compares the slopes. The 95% confidence interval for \\(\\left(\\beta_1 - \\beta_2\\right)\\) is \\[(\\hat{\\beta}_1-\\hat{\\beta}_2) \\pm t(n_1+n_2-4; 0.975)\\sqrt{\\left(\\frac{\\text{RSS}_1+\\text{RSS}_2}{n_1+n_2-4}\\right)\\left(\\frac{1}{S_{x_1x_1}}+\\frac{1}{S_{x_2x_2}}\\right)}\\] To compute the 95% confidence interval we first need to obtain its components. We can obtain \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) from coef(Model1) ## (Intercept) Age statenameNebraska ## 35.8112138 3.2381449 65.4865523 ## Age:statenameNebraska ## -0.7177069 which gives us the model estimates of the coefficients. Here, Iowa is taken as the 'reference' state (due to being coded as 1, while Nebraska is 2), and so its slope parameter, \\(\\hat{\\beta}_1\\), is given by beta1 &lt;- coef(Model1)[2] The number in the square brackets picks out the element of interest. The slope for Nebraska (\\(\\hat{\\beta}_2\\)) is found by adding together the coefficients of age and its state interaction, such that beta2 &lt;- coef(Model1)[2] + coef(Model1)[4] The \\(t\\)-value can be obtained using the commands n1 &lt;- sum(cholest$State == 1) n2 &lt;- sum(cholest$State == 2) p &lt;- 4 t.val &lt;- qt(p = 0.975, df = (n1 + n2 - p)) The residual sum of squares, \\(\\text{RSS} = \\text{RSS}_1 + \\text{RSS}_2\\), can be found from the analysis of variance table using the following command RSS &lt;- anova(Model1)[4, 2] Now, we just need to find the sum of squares, \\(S_{x_1 x_1}\\) and \\(S_{x_2 x_2}\\), for each state, which can be done using x1 &lt;- cholest[cholest$State == 1, 1] x2 &lt;- cholest[cholest$State == 2, 1] xbar1 &lt;- mean(x1) xbar2 &lt;- mean(x2) Sx1x1 &lt;- sum((x1 - xbar1)^2) Sx2x2 &lt;- sum((x2 - xbar2)^2) ese &lt;- sqrt(((RSS)/(n1+n2-p)) * ((1/Sx1x1)+(1/Sx2x2))) We can now obtain the 95% confidence interval for the difference in slopes using the following commands lower &lt;- (beta1-beta2) - t.val * ese upper &lt;- (beta1-beta2) + t.val * ese int &lt;- c(lower, upper) COMMENT: The 95% confidence interval for the difference in slopes is (-1.67, 3.11). What does this tell us about fitting different regression lines for the two states? Does it tell us anything about potential differences between the two states? To compare the parallel lines model and the single regression line model, we compute a 95% confidence interval for the difference between parallel regression lines: \\[ \\hat{\\alpha}_1-\\hat{\\alpha}_2+\\hat{\\beta}(\\bar{x}_{2.}-\\bar{x}_{1.}) \\pm t(n_1 + n_2 - 3,0.975) \\sqrt{\\left(\\frac{\\text{RSS}}{n_1 + n_2 - 3}\\right)\\left(\\frac{1}{n_1}+\\frac{1}{n_2}+\\frac{(\\bar{x}_{2.}-\\bar{x}_{1.})^2}{S_{x_1x_1}+S_{x_2x_2}}\\right)}\\] We need to be careful not to compute the confidence interval in a similar way to the previous one, with just using the coefficient estimates from Model2. This is because the least squares estimates of parameters, given in Introduction (Section 1.1), are derived after reparameterising the model by mean centering age. However, we did not reparameterise Model2, and therefore, the estimates of intercepts, \\(\\hat{\\alpha}_1\\) and \\(\\hat{\\alpha}_2\\), will be different from the formula. One way of reparameterising Model2 can be done by using the following commands: Y &lt;- cholest$Cholesterol Age &lt;- cholest$Age Age[cholest$State == 1] &lt;- Age[cholest$State == 1] - xbar1 Age[cholest$State == 2] &lt;- Age[cholest$State == 2] - xbar2 Model2 &lt;- lm(Y ~ Age + cholest$statename) Here Age is our reparameterised age variable, where we have subtracted from age the corresponding state means. We can now obtain the 95% confidence interval for the difference between parallel regression lines using the following commands: alpha1 &lt;- coef(Model2)[1] alpha2 &lt;- coef(Model2)[1] + coef(Model2)[3] beta &lt;- coef(Model2)[2] p &lt;- 3 t.val &lt;- qt(p = 0.975, df = (n1 + n2 - p)) RSS &lt;- anova(Model2)[3, 2] ese &lt;- sqrt((RSS/(n1+n2-p))*((1/n1)+(1/n2)+((xbar2-xbar1)^2/(Sx1x1+Sx2x2)))) lower &lt;- alpha1-alpha2+(beta*(xbar2-xbar1)) - t.val * ese upper &lt;- alpha1-alpha2+(beta*(xbar2-xbar1)) + t.val * ese int &lt;- c(lower, upper) COMMENT: The 95% confidence interval for the difference between parallel lines is (-62.59, 5.29). What does this tell us about fitting parallel lines for the two states? "],["example-2-respiratory-distress-syndrome.html", "3 Example 2: Respiratory Distress Syndrome", " 3 Example 2: Respiratory Distress Syndrome We have seen this example in Lecture 15. To recap, premature babies often suffer from a variety of problems and respiratory distress syndrome (RDS) is a major one of these problems. It is thought that the occurrence of this syndrome might be related to a property of the blood called red cell deformability. This refers to the ability of red cells to change shape to pass through small pores. The rate (Lrate, on a log scale) of blood flow through a set of 3\\(\\mu\\)m pores has been recorded for two groups of babies, some of whom suffer from respiratory distress syndrome (RDS = 2) and some who do not (RDS = 1). The gestational age (GA) in weeks of each baby is also recorded. These data were kindly provided by Queen Mother's Hospital, Glasgow. Read in the data using: rds &lt;- read.csv(&quot;respiratory.csv&quot;) The dataset contains three columns:    C1: Lrate    C2: GA    C3: RDS (1 = does not suffer from RDS, 2 = suffers from RDS) Use what you have learned from Example 1 to answer the following question of interest. Question of interest: Are there any differences between the rate of blood flow measurements for babies that do, and do not, suffer from RDS, after correcting for the gestational age? TASK: Produce a scatterplot to explore the relationship between Lrate and GA, labelled by RDS. Do you think this relationship differs by RDS? Which model do you think is most appropriate to describe the data? Solution plot(Lrate ~ GA, pch = RDS, data = rds, ylab = &quot;LRate (log rate of blood flow)&quot;, xlab = &quot;Gestational age (Weeks)&quot;) legend(&quot;topleft&quot;, legend = c(&quot;No RDS&quot;, &quot;RDS&quot;), pch = 1:2, bty = &quot;n&quot;, cex = 1.2) Build regression models corresponding to different regression lines, parallel regression lines and a single regression line. Based on the ANOVA table, comment on which model is most appropriate. Hint Recall that the lm command for different regression lines has the form of lm(Y ~ X1 * X2) and that for parallel regression lines has the form of lm(Y ~ X1 + X2). Solution # different lines model Model1 &lt;- lm(Lrate ~ GA * RDS, data = rds) anova(Model1) ## Analysis of Variance Table ## ## Response: Lrate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GA 1 5.9335 5.9335 13.5081 0.001608 ** ## RDS 1 0.0466 0.0466 0.1062 0.748129 ## GA:RDS 1 0.0806 0.0806 0.1836 0.673147 ## Residuals 19 8.3458 0.4393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # parallel lines model Model2 &lt;- lm(Lrate ~ GA + RDS, data = rds) anova(Model2) ## Analysis of Variance Table ## ## Response: Lrate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GA 1 5.9335 5.9335 14.0830 0.001253 ** ## RDS 1 0.0466 0.0466 0.1107 0.742844 ## Residuals 20 8.4264 0.4213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # single regression line model Model3 &lt;- lm(Lrate ~ GA, data = rds) anova(Model3) ## Analysis of Variance Table ## ## Response: Lrate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GA 1 5.9335 5.9335 14.706 0.0009637 *** ## Residuals 21 8.4730 0.4035 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In Model 1, we see that the \\(p\\)-value for GA:RDS is larger than 0.05, meaning that the interaction term is not statistically significant. Therefore, it is inappropriate to fit different regression lines to this dataset. In Model 2, we see that the \\(p\\)-value for RDS is larger than 0.05, meaning that including the factor RDS is not useful in predicting Lrate. In Model 3, we see that the \\(p\\)-value for GA is smaller than 0.05, meaning that including GA is useful in predicting Lrate. Therefore, the most appropriate model, among different regression lines, parallel regression lines and a single regression line, for this dataset is the single regression line model. Looking at the following R output, answer the following questions. Two simple linear regression models are fitted to the data, one for babies who do not suffer from RDS (group 1) and the other one for babies with RDS (group 2), and the associated R output was shown below. ## ## Call: ## lm(formula = Lrate[RDS == 1] ~ GA[RDS == 1]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87508 -0.61871 -0.04343 0.28298 1.43714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.17250 3.17395 -1.630 0.134 ## GA[RDS == 1] 0.17222 0.09791 1.759 0.109 ## ## Residual standard error: 0.7874 on 10 degrees of freedom ## Multiple R-squared: 0.2363, Adjusted R-squared: 0.1599 ## F-statistic: 3.094 on 1 and 10 DF, p-value: 0.1091 ## ## Call: ## lm(formula = Lrate[RDS == 2] ~ GA[RDS == 2]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.83597 -0.16097 0.01831 0.14474 0.79117 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.89113 1.38325 -2.813 0.0203 * ## GA[RDS == 2] 0.12714 0.04818 2.639 0.0270 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4884 on 9 degrees of freedom ## Multiple R-squared: 0.4362, Adjusted R-squared: 0.3736 ## F-statistic: 6.963 on 1 and 9 DF, p-value: 0.02697 Use the R output to deduce the least squares estimates for parameters in the separate regression lines model, i.e. \\(\\hat{\\alpha}_1\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\alpha}_2\\), \\(\\hat{\\alpha}_2\\) in the following model Model: \\(Y_{ij}=\\alpha_i + \\beta_i x_{ij}+\\epsilon_{ij},\\ i=1,2,\\ j=1,\\ldots,n_i\\), Enter your answers to 3 decimal places. \\(\\hat{\\alpha}_1\\) = \\(\\hat{\\beta}_1\\) = \\(\\hat{\\alpha}_2\\) = \\(\\hat{\\beta}_2\\) = The separate regression lines model was fitted to the data and the associated R output was shown below. Model: \\(\\mathbb{E}(Y_{ij})=\\alpha_i + \\beta_i (x_{ij}-\\bar{x}_{i.})\\), where group 1 are babies without RDS and group 2 are babies with RDS. Group 1 is used as the reference level in R and the variable GA has been mean-centred before fitting the model. ## ## Call: ## lm(formula = rds$Lrate ~ GA * as.factor(rds$RDS)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87508 -0.43260 0.00379 0.20733 1.43714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.39583 0.19132 2.069 0.0524 . ## GA 0.17222 0.08242 2.090 0.0503 . ## as.factor(rds$RDS)2 -0.65765 0.27665 -2.377 0.0281 * ## GA:as.factor(rds$RDS)2 -0.04507 0.10521 -0.428 0.6731 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6628 on 19 degrees of freedom ## Multiple R-squared: 0.4207, Adjusted R-squared: 0.3292 ## F-statistic: 4.599 on 3 and 19 DF, p-value: 0.01392 Write down the least squares estimates for parameters in the separate regression lines model. Enter your answers to 3 decimal places. \\(\\hat{\\alpha}_1\\) = \\(\\hat{\\beta}_1\\) = \\(\\hat{\\alpha}_2\\) = \\(\\hat{\\beta}_2\\) = The parallel regression lines model was fitted to the data and the associated R output was shown below. Model: \\(\\mathbb{E}(Y_{ij})=\\alpha_i + \\beta (x_{ij}-\\bar{x}_{i.})\\), where group 1 are babies without RDS and group 2 are babies with RDS. Group 1 is used as the reference level in R and the variable GA has been mean-centred before fitting the model. Model2_b &lt;- lm(rds$Lrate ~ GA + as.factor(rds$RDS)) summary(Model2_b) ## ## Call: ## lm(formula = rds$Lrate ~ GA + as.factor(rds$RDS)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.89309 -0.40617 -0.03309 0.26879 1.48324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.39583 0.18738 2.113 0.04742 * ## GA 0.14455 0.05017 2.881 0.00923 ** ## as.factor(rds$RDS)2 -0.65765 0.27095 -2.427 0.02478 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6491 on 20 degrees of freedom ## Multiple R-squared: 0.4151, Adjusted R-squared: 0.3566 ## F-statistic: 7.097 on 2 and 20 DF, p-value: 0.004686 Write down the least squares estimates for parameters in the separate regression lines model. Enter your answers to 3 decimal places. \\(\\hat{\\alpha}_1\\) = \\(\\hat{\\alpha}_2\\) = \\(\\hat{\\beta}\\) = Construct confidence intervals to compare the models. The 95% confidence interval for \\((\\beta_1-\\beta_2)\\) is (, ). The 95% confidence interval for \\(\\alpha_1 - \\alpha_2 + \\beta \\left(\\bar{x}_{2.} - \\bar{x}_{1.}\\right)\\) is (, ). Among different regression lines, parallel regression lines and a single regression line, the most appropriate model for this dataset is the different regression lines modelthe parallel regression lines modelthe single regression line model. Hint The confidence interval should be constructed for \\((\\beta_1-\\beta_2)\\) in order to compare between different regression lines and parallel regression lines. The confidence interval should be constructed for \\(\\alpha_1 - \\alpha_2 + \\beta \\left(\\bar{x}_{2.} - \\bar{x}_{1.}\\right)\\) in order to compare between parallel regression lines and a single regression line. Solution # different lines model x1 &lt;- rds$GA[rds$RDS==1] x2 &lt;- rds$GA[rds$RDS==2] xbar1 &lt;- mean(x1) xbar2 &lt;- mean(x2) GA &lt;- rds$GA GA[rds$RDS==1] &lt;- GA[rds$RDS==1] - xbar1 GA[rds$RDS==2] &lt;- GA[rds$RDS==2] - xbar2 Model1_b &lt;- lm(rds$Lrate ~ GA * as.factor(rds$RDS)) #Remark 1: GA is the centred covariate, rather than the original rds$GA #Remark 2: RDS is a factor and we need to tell this information to R by using as.factor(); otherwise R will treat it as a continuous variable. summary(Model1_b) ## ## Call: ## lm(formula = rds$Lrate ~ GA * as.factor(rds$RDS)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87508 -0.43260 0.00379 0.20733 1.43714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.39583 0.19132 2.069 0.0524 . ## GA 0.17222 0.08242 2.090 0.0503 . ## as.factor(rds$RDS)2 -0.65765 0.27665 -2.377 0.0281 * ## GA:as.factor(rds$RDS)2 -0.04507 0.10521 -0.428 0.6731 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6628 on 19 degrees of freedom ## Multiple R-squared: 0.4207, Adjusted R-squared: 0.3292 ## F-statistic: 4.599 on 3 and 19 DF, p-value: 0.01392 anova(Model1_b) ## Analysis of Variance Table ## ## Response: rds$Lrate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GA 1 3.4979 3.4979 7.9633 0.01089 * ## as.factor(rds$RDS) 1 2.4822 2.4822 5.6510 0.02811 * ## GA:as.factor(rds$RDS) 1 0.0806 0.0806 0.1836 0.67315 ## Residuals 19 8.3458 0.4393 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # calculate least squares estimates y1 &lt;- rds$Lrate[rds$RDS==1] y2 &lt;- rds$Lrate[rds$RDS==2] ybar1 &lt;- mean(y1) ybar2 &lt;- mean(y2) Sx1x1 &lt;- sum((x1 - xbar1)^2) Sx2x2 &lt;- sum((x2 - xbar2)^2) Sx1y1 &lt;- sum((x1 - xbar1) * (y1 - ybar1)) Sx2y2 &lt;- sum((x2 - xbar2) * (y2 - ybar2)) print(c(ybar1,Sx1y1/Sx1x1,ybar2,Sx2y2/Sx2x2)) ## [1] 0.3958333 0.1722165 -0.2618182 0.1271416 # calculate confidence intervals RSS &lt;- anova(Model1_b)[4,2] n1 &lt;- length(x1) n2 &lt;- length(x2) t.val &lt;- qt(0.975,df=n1+n2-4) ese &lt;- sqrt(RSS/(n1+n2-4)*(1/Sx1x1+1/Sx2x2)) lower &lt;- -coef(Model1_b)[4] - t.val*ese upper &lt;- -coef(Model1_b)[4] + t.val*ese print(c(lower,upper)) ## GA:as.factor(rds$RDS)2 GA:as.factor(rds$RDS)2 ## -0.1751249 0.2652747 The confidence interval for \\((\\beta_1-\\beta_2)\\) includes zero, meaning that it is likely that the interaction term does not have any effect in predicting the response variable. Therefore, we would prefer parallel regression lines over different regression lines. # parallel lines model Model2_b &lt;- lm(rds$Lrate ~ GA + as.factor(rds$RDS)) summary(Model2_b) ## ## Call: ## lm(formula = rds$Lrate ~ GA + as.factor(rds$RDS)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.89309 -0.40617 -0.03309 0.26879 1.48324 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.39583 0.18738 2.113 0.04742 * ## GA 0.14455 0.05017 2.881 0.00923 ** ## as.factor(rds$RDS)2 -0.65765 0.27095 -2.427 0.02478 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6491 on 20 degrees of freedom ## Multiple R-squared: 0.4151, Adjusted R-squared: 0.3566 ## F-statistic: 7.097 on 2 and 20 DF, p-value: 0.004686 anova(Model2_b) ## Analysis of Variance Table ## ## Response: rds$Lrate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GA 1 3.4979 3.4979 8.3022 0.00923 ** ## as.factor(rds$RDS) 1 2.4822 2.4822 5.8915 0.02478 * ## Residuals 20 8.4264 0.4213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # calculate least squares estimates print(c(ybar1,ybar2,(Sx1y1+Sx2y2)/(Sx1x1+Sx2x2))) #least squares estimates based on formulae ## [1] 0.3958333 -0.2618182 0.1445547 Coef &lt;-coef(Model2_b) print(c(Coef[1],Coef[1]+Coef[3],Coef[2])) #read least squares estimates from R ## (Intercept) (Intercept) GA ## 0.3958333 -0.2618182 0.1445547 # calculate confidence intervals Sy1y1 &lt;- sum((y1 - ybar1)^2) Sy2y2 &lt;- sum((y2 - ybar2)^2) RSS &lt;- anova(Model2_b)[3,2] # Sy1y1 + Sy2y2 - (Sx1y1+Sx2y2)^2/(Sx1x1+Sx2x2) #RSS based on formula t.val &lt;- qt(0.975, df = n1+n2-3) ese &lt;- sqrt(RSS/(n1+n2-3)*(1/n1+1/n2+(xbar2-xbar1)^2/(Sx1x1+Sx2x2))) lower &lt;- -coef(Model2_b)[3] + coef(Model2_b)[2]*(xbar2-xbar1) - t.val*ese upper &lt;- -coef(Model2_b)[3] + coef(Model2_b)[2]*(xbar2-xbar1) + t.val*ese print(c(lower,upper)) ## as.factor(rds$RDS)2 as.factor(rds$RDS)2 ## -0.5802435 0.8004354 The confidence interval for \\(\\alpha_1 - \\alpha_2 + \\beta \\left(\\bar{x}_{2.} - \\bar{x}_{1.}\\right)\\) includes zero, meaning that it is likely that RDS does not have any effect in predicting Lrate. Therefore, we would prefer a single regression line over parallel regression lines. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
